# train_fusion_cnn.py
# ------------------------------------------------------------
# Train Fusion CNN on offline cache generated by make_fusion_cache.py
# (Unified pipeline for clean / rain / fog)
#
# Modifications:
#   - Epochs -> 100
#   - Early Stopping Patience -> 100 (Disable early stopping effectively)
#   - LR Scheduler Patience -> 10 (Slower decay)
# ------------------------------------------------------------

import os
import json
import random
import numpy as np
from tqdm import tqdm

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

from cnn_model import HorizonResNet


# =========================
# Config
# =========================
# Cache root produced by the NEW make_fusion_cache.py
CACHE_ROOT = r"Hashmani's Dataset/FusionCache_1024x576"
TRAIN_CACHE_DIR = os.path.join(CACHE_ROOT, "train")
VAL_CACHE_DIR   = os.path.join(CACHE_ROOT, "val")
TEST_CACHE_DIR  = os.path.join(CACHE_ROOT, "test")

# Fixed split indices (row indices of GroundTruth.csv)
SPLIT_DIR = r"splits_musid"

SEED = 40
BATCH_SIZE = 16  # CNN 比较轻，Batch Size 可以大一点，比如 16 或 32
NUM_WORKERS = 4

# [修改点 1] 训练轮数
NUM_EPOCHS = 100 
LR = 2e-4
WEIGHT_DECAY = 1e-4

USE_AMP = True
GRAD_CLIP_NORM = 1.0

# [修改点 2] 学习率调度与早停策略调整
# 既然要跑满 100 轮，就把耐心值调大，不要太快降学习率，也不要太早停止
PLATEAU_PATIENCE = 10      # 10个epoch不降才减小LR
PLATEAU_FACTOR = 0.5       # 每次减半
EARLY_STOP_PATIENCE = 100  # 设为 100，实际上就是禁用了早停，保证跑满

# Model / log outputs
BEST_PATH = "weights/best_fusion_cnn_1024x576.pth"
OUT_JSON  = os.path.join(SPLIT_DIR, "train_fusion_cnn_1024x576.json")

# Dataset fallback shape (should match cache input shape)
FALLBACK_SHAPE = (4, 2240, 180)

# =========================
# Data augmentation (ship-line / spurious straight-line interference)
# =========================
AUG_ENABLE = True
AUG_SPURIOUS_P = 0.60          # probability to inject spurious Radon peaks (train only)
AUG_MAX_PEAKS = 3              # how many spurious peaks to inject per sample
AUG_AMP_MIN, AUG_AMP_MAX = 0.15, 0.60
AUG_SIGMA_RHO = 18.0           # gaussian radius along rho axis (pixels)
AUG_SIGMA_THETA = 1.8          # gaussian radius along theta axis (bins)
AUG_TARGET_CHANNELS = (0, 1, 2)  # default: only traditional Radon channels


DEVICE = "cuda" if torch.cuda.is_available() else "cpu"


# =========================
# Reproducibility
# =========================
def seed_everything(seed: int):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def ensure_dir(p: str):
    os.makedirs(p, exist_ok=True)


# =========================
# Split loader
# =========================
def load_split_indices(split_dir: str):
    primary = {
        "train": os.path.join(split_dir, "train_indices.npy"),
        "val":   os.path.join(split_dir, "val_indices.npy"),
        "test":  os.path.join(split_dir, "test_indices.npy"),
    }
    if all(os.path.exists(p) for p in primary.values()):
        return {
            "train": np.load(primary["train"]).astype(np.int64).tolist(),
            "val":   np.load(primary["val"]).astype(np.int64).tolist(),
            "test":  np.load(primary["test"]).astype(np.int64).tolist(),
        }
    raise FileNotFoundError(f"Cannot find split indices in {split_dir}")


# =========================
# Dataset
# =========================
def _inject_gaussian_peak(x: torch.Tensor, ch: int, rho0: int, th0: int, amp: float,
                          sigma_rho: float, sigma_th: float) -> None:
    """In-place add a small 2D Gaussian peak to x[ch] at (rho0, th0).

    This is used to simulate strong non-horizon straight-line responses (e.g., ship decks/masts)
    that can create competing peaks in the Radon domain.
    """
    _, H, W = x.shape
    # local window for efficiency
    sr = int(max(3, min(H // 2, round(3.0 * sigma_rho))))
    st = int(max(2, min(W // 2, round(3.0 * sigma_th))))
    r1 = max(0, rho0 - sr)
    r2 = min(H, rho0 + sr + 1)
    t1 = max(0, th0 - st)
    t2 = min(W, th0 + st + 1)

    rr = torch.arange(r1, r2, device=x.device, dtype=x.dtype) - float(rho0)
    tt = torch.arange(t1, t2, device=x.device, dtype=x.dtype) - float(th0)
    gr = torch.exp(-(rr * rr) / (2.0 * sigma_rho * sigma_rho))
    gt = torch.exp(-(tt * tt) / (2.0 * sigma_th * sigma_th))
    g2d = gr[:, None] * gt[None, :]
    x[ch, r1:r2, t1:t2] = torch.clamp(x[ch, r1:r2, t1:t2] + amp * g2d, 0.0, 1.0)


def augment_fusion_tensor(x: torch.Tensor) -> torch.Tensor:
    """On-the-fly augmentation for FusionCache tensors.

    x: [C, H, W], values assumed in [0,1].
    """
    if (not AUG_ENABLE) or (torch.rand(1).item() > AUG_SPURIOUS_P):
        return x

    C, H, W = x.shape
    n_peaks = int(torch.randint(1, AUG_MAX_PEAKS + 1, (1,)).item())
    for _ in range(n_peaks):
        ch = int(AUG_TARGET_CHANNELS[int(torch.randint(0, len(AUG_TARGET_CHANNELS), (1,)).item())])
        rho0 = int(torch.randint(0, H, (1,)).item())
        th0 = int(torch.randint(0, W, (1,)).item())
        amp = float(AUG_AMP_MIN + (AUG_AMP_MAX - AUG_AMP_MIN) * torch.rand(1).item())
        _inject_gaussian_peak(x, ch, rho0, th0, amp, AUG_SIGMA_RHO, AUG_SIGMA_THETA)
    return x

class SplitCacheDataset(Dataset):
    def __init__(self, cache_dir: str, indices: list, fallback_shape=(4, 2240, 180), strict_missing: bool = True, augment: bool = False):
        self.cache_dir = cache_dir
        self.indices = list(indices)
        self.fallback_shape = tuple(fallback_shape)
        self.strict_missing = bool(strict_missing)
        self.augment = bool(augment)

    def __len__(self):
        return len(self.indices)

    def __getitem__(self, i: int):
        idx = int(self.indices[i])
        path = os.path.join(self.cache_dir, f"{idx}.npy")
        if not os.path.exists(path):
            if self.strict_missing:
                raise FileNotFoundError(f"Missing cache file: {path}")
            x = torch.zeros(self.fallback_shape, dtype=torch.float32)
            y = torch.zeros(2, dtype=torch.float32)
            return x, y

        data = np.load(path, allow_pickle=True).item()
        x = torch.from_numpy(data["input"]).float()   # [C,H,W]
        y = torch.from_numpy(data["label"]).float()   # [2]
        if self.augment:
            x = augment_fusion_tensor(x)
        return x, y


# =========================
# Loss (rho linear, theta periodic)
# =========================
class HorizonPeriodicLoss(nn.Module):
    def __init__(self, rho_weight=1.0, theta_weight=2.0, rho_beta=0.02, theta_beta=0.02):
        super().__init__()
        self.rho_weight = float(rho_weight)
        self.theta_weight = float(theta_weight)
        self.rho_loss = nn.SmoothL1Loss(beta=rho_beta)
        self.theta_loss = nn.SmoothL1Loss(beta=theta_beta)

    def forward(self, preds, targets):
        loss_rho = self.rho_loss(preds[:, 0], targets[:, 0])

        theta_p = preds[:, 1] * np.pi
        theta_t = targets[:, 1] * np.pi
        sin_p, cos_p = torch.sin(theta_p), torch.cos(theta_p)
        sin_t, cos_t = torch.sin(theta_t), torch.cos(theta_t)

        loss_theta = self.theta_loss(sin_p, sin_t) + self.theta_loss(cos_p, cos_t)
        return self.rho_weight * loss_rho + self.theta_weight * loss_theta


# =========================
# AMP helpers
# =========================
try:
    import torch.amp as torch_amp
    _HAS_TORCH_AMP = True
except Exception:
    _HAS_TORCH_AMP = False
    torch_amp = None

def autocast_ctx():
    if not USE_AMP or not DEVICE.startswith("cuda"):
        from contextlib import nullcontext
        return nullcontext()
    if _HAS_TORCH_AMP:
        return torch_amp.autocast(device_type="cuda", enabled=True)
    return torch.cuda.amp.autocast(enabled=True)

def make_scaler():
    if not USE_AMP or not DEVICE.startswith("cuda"):
        return None
    if _HAS_TORCH_AMP:
        try:
            return torch_amp.GradScaler(device="cuda", enabled=True)
        except TypeError:
            return torch_amp.GradScaler(enabled=True)
    return torch.cuda.amp.GradScaler(enabled=True)


# =========================
# Train / Eval
# =========================
@torch.no_grad()
def evaluate(model, loader, criterion):
    model.eval()
    total_loss = 0.0
    n = 0
    for x, y in loader:
        x = x.to(DEVICE, non_blocking=True)
        y = y.to(DEVICE, non_blocking=True)
        with autocast_ctx():
            pred = model(x)
            loss = criterion(pred, y)
        total_loss += float(loss.item()) * x.size(0)
        n += x.size(0)
    return total_loss / max(1, n)


def train_one_epoch(model, loader, optimizer, scaler, criterion):
    model.train()
    total_loss = 0.0
    n = 0

    for x, y in tqdm(loader, desc="train", ncols=90):
        x = x.to(DEVICE, non_blocking=True)
        y = y.to(DEVICE, non_blocking=True)

        optimizer.zero_grad(set_to_none=True)

        if scaler is not None:
            with autocast_ctx():
                pred = model(x)
                loss = criterion(pred, y)
            scaler.scale(loss).backward()
            if GRAD_CLIP_NORM and GRAD_CLIP_NORM > 0:
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP_NORM)
            scaler.step(optimizer)
            scaler.update()
        else:
            pred = model(x)
            loss = criterion(pred, y)
            loss.backward()
            if GRAD_CLIP_NORM and GRAD_CLIP_NORM > 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP_NORM)
            optimizer.step()

        total_loss += float(loss.item()) * x.size(0)
        n += x.size(0)

    return total_loss / max(1, n)


# =========================
# Main
# =========================
def main():
    seed_everything(SEED)
    ensure_dir(SPLIT_DIR)
    ensure_dir("weights")  # 确保权重目录存在

    # sanity check dirs
    for d in [TRAIN_CACHE_DIR, VAL_CACHE_DIR]:
        if not os.path.isdir(d):
            raise FileNotFoundError(f"Cache dir not found: {d}. Please run make_fusion_cache.py first.")

    splits = load_split_indices(SPLIT_DIR)
    train_indices = splits["train"]
    val_indices = splits.get("val", [])
    test_indices = splits.get("test", [])

    if len(val_indices) == 0:
        raise RuntimeError("val_indices is empty.")

    # dataset/loader
    train_ds = SplitCacheDataset(TRAIN_CACHE_DIR, train_indices, fallback_shape=FALLBACK_SHAPE, strict_missing=True, augment=True)
    val_ds   = SplitCacheDataset(VAL_CACHE_DIR,   val_indices,   fallback_shape=FALLBACK_SHAPE, strict_missing=True)
    test_ds  = SplitCacheDataset(TEST_CACHE_DIR,  test_indices,  fallback_shape=FALLBACK_SHAPE, strict_missing=True) if os.path.isdir(TEST_CACHE_DIR) else None

    pin = DEVICE.startswith("cuda")
    train_loader = DataLoader(
        train_ds, batch_size=BATCH_SIZE, shuffle=True,
        num_workers=NUM_WORKERS, pin_memory=pin, drop_last=True
    )
    val_loader = DataLoader(
        val_ds, batch_size=BATCH_SIZE, shuffle=False,
        num_workers=NUM_WORKERS, pin_memory=pin
    )
    test_loader = DataLoader(
        test_ds, batch_size=BATCH_SIZE, shuffle=False,
        num_workers=NUM_WORKERS, pin_memory=pin
    ) if test_ds is not None else None

    # model
    x0, _ = train_ds[0]
    in_ch = int(x0.shape[0])
    model = HorizonResNet(in_channels=in_ch).to(DEVICE)

    criterion = HorizonPeriodicLoss()
    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)
    scaler = make_scaler()

    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode="min", factor=PLATEAU_FACTOR, patience=PLATEAU_PATIENCE, verbose=True
    )

    best_val = float("inf")
    best_epoch = 0
    bad_epochs = 0
    history = []

    print(f"[INFO] Start Training CNN (100 Epochs Mode)")
    print(f"[INFO] Train={len(train_ds)}  Val={len(val_ds)}  Test={(len(test_ds) if test_ds is not None else 0)}")

    for epoch in range(1, NUM_EPOCHS + 1):
        tr_loss = train_one_epoch(model, train_loader, optimizer, scaler, criterion)
        va_loss = evaluate(model, val_loader, criterion)

        lr_now = optimizer.param_groups[0]["lr"]
        print(f"Epoch [{epoch:03d}/{NUM_EPOCHS}]  lr={lr_now:.2e}  train_loss={tr_loss:.6f}  val_loss={va_loss:.6f}")

        history.append({"epoch": epoch, "lr": lr_now, "train_loss": tr_loss, "val_loss": va_loss})

        scheduler.step(va_loss)

        # 保存最佳模型
        if va_loss < best_val - 1e-8:
            best_val = va_loss
            best_epoch = epoch
            bad_epochs = 0
            torch.save(model.state_dict(), BEST_PATH)
            print(f"  -> best updated: {best_val:.6f} (epoch={best_epoch})")
        else:
            bad_epochs += 1
            # 这里的 Early Stop 实际上被禁用了 (直到 100 epoch 不降才停)
            if bad_epochs >= EARLY_STOP_PATIENCE:
                print(f"[EARLY STOP] no improvement for {EARLY_STOP_PATIENCE} epochs. best_epoch={best_epoch}")
                break

    # final eval
    final_test = None
    if test_loader is not None and os.path.exists(BEST_PATH):
        model.load_state_dict(torch.load(BEST_PATH, map_location=DEVICE))
        final_test = evaluate(model, test_loader, criterion)
        print(f"[FINAL TEST] loss={final_test:.6f}")
    
    # save log
    payload = {
        "best_val_loss": best_val,
        "best_epoch": best_epoch,
        "test_loss": final_test,
        "history": history,
    }
    with open(OUT_JSON, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)
    print(f"[Done] Log saved to {OUT_JSON}")


if __name__ == "__main__":
    main()